{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dualcodec\n",
    "import torchaudio\n",
    "import torch\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"12hz_v1\"\n",
    "path = \"/home/vansh/dualcodec-exp/output_checkpoints/dualcodec_25hzv1_finetune/checkpoint/epoch-0022_step-0110600_loss-70.316910-dualcodec_25hzv1_finetune\"\n",
    "name = \"model.safetensors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dualcodec_model = dualcodec.get_model(model_id, path, name=name)\n",
    "dualcodec_inference = dualcodec.Inference(dualcodec_model=dualcodec_model, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id_base = \"12hz_v1\"\n",
    "dualcodec_model_base = dualcodec.get_model(model_id_base)\n",
    "dualcodec_inference_base = dualcodec.Inference(dualcodec_model=dualcodec_model_base, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_tara, sr = torchaudio.load(\"audio_samples/tara.wav\")\n",
    "audio_tara = torchaudio.functional.resample(audio_tara, sr, 24000)\n",
    "audio_tara = audio_tara.reshape(1,1,-1)\n",
    "Audio(audio_tara.squeeze(0), rate=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_tara = audio_tara.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_codes, acoustic_codes = dualcodec_inference.encode(audio_tara, n_quantizers=3)\n",
    "out_audio = dualcodec_inference.decode(semantic_codes, acoustic_codes)\n",
    "print(\"*\")\n",
    "Audio(out_audio.cpu().squeeze(0), rate=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_sam, sr = torchaudio.load(\"audio_samples/sam.wav\")\n",
    "audio_sam = torchaudio.functional.resample(audio_sam, sr, 24000)\n",
    "audio_sam = audio_sam.reshape(1,1,-1)\n",
    "Audio(audio_sam.squeeze(0), rate=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_sam = audio_sam.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_codes, acoustic_codes = dualcodec_inference_base.encode(audio_sam, n_quantizers=8)\n",
    "out_audio = dualcodec_inference_base.decode(semantic_codes, acoustic_codes)\n",
    "print(\"*\")\n",
    "Audio(out_audio.cpu().squeeze(0), rate=24000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "local_dir = \"/mnt/disks/emilia/emilia_dataset/Emilia/EN\"\n",
    "tar_paths = [filename for filename in os.listdir(local_dir) if filename.endswith(\".tar\")]\n",
    "language = \"EN\"\n",
    "            \n",
    "dataset = load_dataset(\n",
    "    local_dir,\n",
    "    data_files={language.lower(): tar_paths},\n",
    "    split=language.lower(),\n",
    "    num_proc=40,\n",
    "    cache_dir=\"/mnt/disks/emilia/emilia_cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"vanshjjw/amu-pushed-luna-4500r\",\n",
    "    split=\"train\",\n",
    ")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for example in ds:\n",
    "    if \"audio\" not in example or example[\"audio\"] is None:\n",
    "        continue\n",
    "    audio_dict = example[\"audio\"]\n",
    "    waveform = torch.tensor(audio_dict[\"array\"], dtype=torch.float32).unsqueeze(0)\n",
    "    sr = int(audio_dict[\"sampling_rate\"])\n",
    "    \n",
    "    if sr != 24000:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, 24000)\n",
    "    \n",
    "    samples.append(\n",
    "        {\n",
    "            \"mp3\": {\n",
    "                \"array\": waveform.numpy(),\n",
    "                \"sampling_rate\": 24000,\n",
    "            },\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 5\n",
    "audio_luna, sr = samples[index]['mp3']['array'], samples[index]['mp3']['sampling_rate']\n",
    "audio_luna = torchaudio.functional.resample(audio_luna, sr, 24000)\n",
    "audio_luna = audio_luna.reshape(1,1,-1)\n",
    "Audio(audio_luna.squeeze(0), rate=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_luna = torch.tensor(audio_luna).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_codes, acoustic_codes = dualcodec_inference.encode(audio_luna, n_quantizers=3)\n",
    "out_audio = dualcodec_inference.decode(semantic_codes, acoustic_codes)\n",
    "print(\"*\")\n",
    "Audio(out_audio.cpu().squeeze(0), rate=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_codes_base, acoustic_codes_base = dualcodec_inference_base.encode(audio_luna, n_quantizers=2)\n",
    "out_audio_base = dualcodec_inference_base.decode(semantic_codes_base, acoustic_codes_base)\n",
    "Audio(out_audio_base.cpu().squeeze(0), rate=24000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
