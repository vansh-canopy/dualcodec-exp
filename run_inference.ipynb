{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dualcodec\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"12hz_v1\" # select from available Model_IDs, \"12hz_v1\" or \"25hz_v1\"\n",
    "dualcodec_model_path = \"./output_checkpoints/dualcodec_12hz_16384_4096_8vq_scratch/checkpoint/epoch-0003_step-0016800_loss-100.451950-dualcodec_12hz_16384_4096_8vq_scratch\"\n",
    "dualcodec_model = dualcodec.get_model(model_id, dualcodec_model_path)\n",
    "dualcodec_inference = dualcodec.Inference(dualcodec_model=dualcodec_model, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do inference for your wav\n",
    "import torchaudio\n",
    "audio, sr = torchaudio.load(\"tara.wav\")\n",
    "# resample to 24kHz\n",
    "audio = torchaudio.functional.resample(audio, sr, 24000)\n",
    "audio = audio.reshape(1,1,-1)\n",
    "audio = audio.to(\"cuda\")\n",
    "# extract codes, for example, using 8 quantizers here:\n",
    "semantic_codes, acoustic_codes = dualcodec_inference.encode(audio, n_quantizers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic_codes shape: torch.Size([B, 1, T])\n",
    "# acoustic_codes shape: torch.Size([B, n_quantizers-1, T])\n",
    "\n",
    "\n",
    "\n",
    "# produce output audio\n",
    "out_audio = dualcodec_inference.decode(semantic_codes, acoustic_codes)\n",
    "\n",
    "# save output audio\n",
    "torchaudio.save(\"out.wav\", out_audio.cpu().squeeze(0), 24000)\n",
    "from IPython.display import Audio\n",
    "Audio(\"out.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size = 4\n",
    "gaussian_std = 0.1  # Controllable parameter for Gaussian standard deviation\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Crop the tensors so they fit in group size\n",
    "semantic_codes_cropped = semantic_codes[:, :, :semantic_codes.shape[2] - semantic_codes.shape[2] % group_size]\n",
    "acoustic_codes_cropped = acoustic_codes[:, :, :acoustic_codes.shape[2] - acoustic_codes.shape[2] % group_size]\n",
    "\n",
    "# Create Gaussian window\n",
    "def create_gaussian_window(length, std_ratio=0.5):\n",
    "    \"\"\"Create a Gaussian window centered at length/2\"\"\"\n",
    "    x = torch.arange(length, dtype=torch.float32)\n",
    "    center = (length - 1) / 2\n",
    "    std = length * std_ratio  # std as a ratio of window length\n",
    "    window = torch.exp(-0.5 * ((x - center) / std) ** 2)\n",
    "    return window\n",
    "\n",
    "# First pass: decode all segments to get their actual sizes\n",
    "all_decoded = []\n",
    "print(\"First pass: decoding all segments...\")\n",
    "for i in range(acoustic_codes.shape[2] - group_size + 1):\n",
    "    semantic_codes_segment = semantic_codes_cropped[:, :, i:i + group_size]\n",
    "    acoustic_codes_segment = acoustic_codes_cropped[:, :, i:i + group_size]\n",
    "    \n",
    "    start_time = time.monotonic()\n",
    "    out_audio = dualcodec_inference.decode(semantic_codes_segment, acoustic_codes_segment)\n",
    "    end_time = time.monotonic()\n",
    "    print(f\"Window {i}/{acoustic_codes.shape[2] - group_size}, decode time: {end_time-start_time:.3f}s, shape: {out_audio.shape}\")\n",
    "    \n",
    "    all_decoded.append(out_audio)\n",
    "\n",
    "# Calculate total output length based on actual decoded sizes\n",
    "# Assuming each window shifts by one sample in the input\n",
    "if len(all_decoded) > 0:\n",
    "    # Estimate samples per input step from first few windows\n",
    "    if len(all_decoded) > 1:\n",
    "        samples_per_step = all_decoded[0].shape[2] // group_size\n",
    "    else:\n",
    "        samples_per_step = all_decoded[0].shape[2] // group_size\n",
    "    \n",
    "    # Calculate total length\n",
    "    total_output_length = sum(audio.shape[2] for audio in all_decoded[:1])  # First chunk full size\n",
    "    total_output_length += (len(all_decoded) - 1) * samples_per_step  # Remaining chunks shifted\n",
    "    \n",
    "    # Initialize output tensor and weight accumulator\n",
    "    device = all_decoded[0].device\n",
    "    output_audio = torch.zeros(1, 1, total_output_length, device=device)\n",
    "    weight_sum = torch.zeros(1, 1, total_output_length, device=device)\n",
    "    \n",
    "    # Second pass: apply windows and accumulate\n",
    "    print(\"\\nSecond pass: applying crossfade...\")\n",
    "    for i, out_audio in enumerate(all_decoded):\n",
    "        current_samples = out_audio.shape[2]\n",
    "        \n",
    "        # Create Gaussian window for this specific chunk size\n",
    "        gaussian_window = create_gaussian_window(current_samples, std_ratio=gaussian_std)\n",
    "        gaussian_window = gaussian_window.to(device).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Apply Gaussian window to the decoded audio\n",
    "        windowed_audio = out_audio * gaussian_window\n",
    "        \n",
    "        # Calculate position in output\n",
    "        output_start = i * samples_per_step\n",
    "        output_end = min(output_start + current_samples, total_output_length)\n",
    "        actual_samples = output_end - output_start\n",
    "        \n",
    "        # Handle case where chunk might be larger than remaining space\n",
    "        if actual_samples < current_samples:\n",
    "            windowed_audio = windowed_audio[:, :, :actual_samples]\n",
    "            gaussian_window = gaussian_window[:, :, :actual_samples]\n",
    "        \n",
    "        # Add to output with overlap\n",
    "        output_audio[:, :, output_start:output_end] += windowed_audio\n",
    "        weight_sum[:, :, output_start:output_end] += gaussian_window\n",
    "        \n",
    "        print(f\"Window {i}: position {output_start}-{output_end}, chunk size: {current_samples}\")\n",
    "    \n",
    "    # Normalize by the sum of weights to maintain consistent amplitude\n",
    "    # Add small epsilon to avoid division by zero\n",
    "    output_audio = output_audio / (weight_sum + 1e-8)\n",
    "    \n",
    "    # Trim any trailing zeros if we overestimated length\n",
    "    # Find last non-zero sample\n",
    "    non_zero_indices = torch.nonzero(weight_sum.squeeze() > 0.01)\n",
    "    if len(non_zero_indices) > 0:\n",
    "        last_valid_idx = non_zero_indices[-1].item() + 1\n",
    "        output_audio = output_audio[:, :, :last_valid_idx]\n",
    "    \n",
    "    # Convert to numpy and display\n",
    "    display(Audio(output_audio.cpu().squeeze(0).squeeze(0).numpy(), rate=24000))\n",
    "else:\n",
    "    print(\"No audio decoded!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
