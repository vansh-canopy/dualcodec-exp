{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e1e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import dualcodec\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plotter\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59daa7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 60661.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/vansh/dualcodec-exp/.venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /home/vansh/.cache/huggingface/hub/models--amphion--dualcodec/snapshots/b5d3158cbd1007441794398435438228f1e80c28/dualcodec_12hz_16384_4096.safetensors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DualCodec:\n    Missing key(s) in state_dict: \"dac.decoder.model.1.block.1.conv.conv.bias\", \"dac.decoder.model.1.block.1.conv.conv.weight_g\", \"dac.decoder.model.1.block.1.conv.conv.weight_v\", \"dac.decoder.model.1.block.2.block.1.conv.bias\", \"dac.decoder.model.1.block.2.block.1.conv.weight_g\", \"dac.decoder.model.1.block.2.block.1.conv.weight_v\", \"dac.decoder.model.1.block.2.block.3.conv.bias\", \"dac.decoder.model.1.block.2.block.3.conv.weight_g\", \"dac.decoder.model.1.block.2.block.3.conv.weight_v\", \"dac.decoder.model.1.block.3.block.1.conv.bias\", \"dac.decoder.model.1.block.3.block.1.conv.weight_g\", \"dac.decoder.model.1.block.3.block.1.conv.weight_v\", \"dac.decoder.model.1.block.3.block.3.conv.bias\", \"dac.decoder.model.1.block.3.block.3.conv.weight_g\", \"dac.decoder.model.1.block.3.block.3.conv.weight_v\", \"dac.decoder.model.1.block.4.block.1.conv.bias\", \"dac.decoder.model.1.block.4.block.1.conv.weight_g\", \"dac.decoder.model.1.block.4.block.1.conv.weight_v\", \"dac.decoder.model.1.block.4.block.3.conv.bias\", \"dac.decoder.model.1.block.4.block.3.conv.weight_g\", \"dac.decoder.model.1.block.4.block.3.conv.weight_v\", \"dac.decoder.model.2.block.1.conv.conv.bias\", \"dac.decoder.model.2.block.1.conv.conv.weight_g\", \"dac.decoder.model.2.block.1.conv.conv.weight_v\", \"dac.decoder.model.2.block.2.block.1.conv.bias\", \"dac.decoder.model.2.block.2.block.1.conv.weight_g\", \"dac.decoder.model.2.block.2.block.1.conv.weight_v\", \"dac.decoder.model.2.block.2.block.3.conv.bias\", \"dac.decoder.model.2.block.2.block.3.conv.weight_g\", \"dac.decoder.model.2.block.2.block.3.conv.weight_v\", \"dac.decoder.model.2.block.3.block.1.conv.bias\", \"dac.decoder.model.2.block.3.block.1.conv.weight_g\", \"dac.decoder.model.2.block.3.block.1.conv.weight_v\", \"dac.decoder.model.2.block.3.block.3.conv.bias\", \"dac.decoder.model.2.block.3.block.3.conv.weight_g\", \"dac.decoder.model.2.block.3.block.3.conv.weight_v\", \"dac.decoder.model.2.block.4.block.1.conv.bias\", \"dac.decoder.model.2.block.4.block.1.conv.weight_g\", \"dac.decoder.model.2.block.4.block.1.conv.weight_v\", \"dac.decoder.model.2.block.4.block.3.conv.bias\", \"dac.decoder.model.2.block.4.block.3.conv.weight_g\", \"dac.decoder.model.2.block.4.block.3.conv.weight_v\", \"dac.decoder.model.3.block.1.conv.conv.bias\", \"dac.decoder.model.3.block.1.conv.conv.weight_g\", \"dac.decoder.model.3.block.1.conv.conv.weight_v\", \"dac.decoder.model.3.block.2.block.1.conv.bias\", \"dac.decoder.model.3.block.2.block.1.conv.weight_g\", \"dac.decoder.model.3.block.2.block.1.conv.weight_v\", \"dac.decoder.model.3.block.2.block.3.conv.bias\", \"dac.decoder.model.3.block.2.block.3.conv.weight_g\", \"dac.decoder.model.3.block.2.block.3.conv.weight_v\", \"dac.decoder.model.3.block.3.block.1.conv.bias\", \"dac.decoder.model.3.block.3.block.1.conv.weight_g\", \"dac.decoder.model.3.block.3.block.1.conv.weight_v\", \"dac.decoder.model.3.block.3.block.3.conv.bias\", \"dac.decoder.model.3.block.3.block.3.conv.weight_g\", \"dac.decoder.model.3.block.3.block.3.conv.weight_v\", \"dac.decoder.model.3.block.4.block.1.conv.bias\", \"dac.decoder.model.3.block.4.block.1.conv.weight_g\", \"dac.decoder.model.3.block.4.block.1.conv.weight_v\", \"dac.decoder.model.3.block.4.block.3.conv.bias\", \"dac.decoder.model.3.block.4.block.3.conv.weight_g\", \"dac.decoder.model.3.block.4.block.3.conv.weight_v\", \"dac.decoder.model.4.block.1.conv.conv.bias\", \"dac.decoder.model.4.block.1.conv.conv.weight_g\", \"dac.decoder.model.4.block.1.conv.conv.weight_v\", \"dac.decoder.model.4.block.2.block.1.conv.bias\", \"dac.decoder.model.4.block.2.block.1.conv.weight_g\", \"dac.decoder.model.4.block.2.block.1.conv.weight_v\", \"dac.decoder.model.4.block.2.block.3.conv.bias\", \"dac.decoder.model.4.block.2.block.3.conv.weight_g\", \"dac.decoder.model.4.block.2.block.3.conv.weight_v\", \"dac.decoder.model.4.block.3.block.1.conv.bias\", \"dac.decoder.model.4.block.3.block.1.conv.weight_g\", \"dac.decoder.model.4.block.3.block.1.conv.weight_v\", \"dac.decoder.model.4.block.3.block.3.conv.bias\", \"dac.decoder.model.4.block.3.block.3.conv.weight_g\", \"dac.decoder.model.4.block.3.block.3.conv.weight_v\", \"dac.decoder.model.4.block.4.block.1.conv.bias\", \"dac.decoder.model.4.block.4.block.1.conv.weight_g\", \"dac.decoder.model.4.block.4.block.1.conv.weight_v\", \"dac.decoder.model.4.block.4.block.3.conv.bias\", \"dac.decoder.model.4.block.4.block.3.conv.weight_g\", \"dac.decoder.model.4.block.4.block.3.conv.weight_v\", \"dac.decoder.model.5.block.1.conv.conv.bias\", \"dac.decoder.model.5.block.1.conv.conv.weight_g\", \"dac.decoder.model.5.block.1.conv.conv.weight_v\", \"dac.decoder.model.5.block.2.block.1.conv.bias\", \"dac.decoder.model.5.block.2.block.1.conv.weight_g\", \"dac.decoder.model.5.block.2.block.1.conv.weight_v\", \"dac.decoder.model.5.block.2.block.3.conv.bias\", \"dac.decoder.model.5.block.2.block.3.conv.weight_g\", \"dac.decoder.model.5.block.2.block.3.conv.weight_v\", \"dac.decoder.model.5.block.3.block.1.conv.bias\", \"dac.decoder.model.5.block.3.block.1.conv.weight_g\", \"dac.decoder.model.5.block.3.block.1.conv.weight_v\", \"dac.decoder.model.5.block.3.block.3.conv.bias\", \"dac.decoder.model.5.block.3.block.3.conv.weight_g\", \"dac.decoder.model.5.block.3.block.3.conv.weight_v\", \"dac.decoder.model.5.block.4.block.1.conv.bias\", \"dac.decoder.model.5.block.4.block.1.conv.weight_g\", \"dac.decoder.model.5.block.4.block.1.conv.weight_v\", \"dac.decoder.model.5.block.4.block.3.conv.bias\", \"dac.decoder.model.5.block.4.block.3.conv.weight_g\", \"dac.decoder.model.5.block.4.block.3.conv.weight_v\"\n    Unexpected key(s) in state_dict: \"dac.decoder.model.1.block.1.bias\", \"dac.decoder.model.1.block.1.weight_g\", \"dac.decoder.model.1.block.1.weight_v\", \"dac.decoder.model.1.block.2.block.1.bias\", \"dac.decoder.model.1.block.2.block.1.weight_g\", \"dac.decoder.model.1.block.2.block.1.weight_v\", \"dac.decoder.model.1.block.2.block.3.bias\", \"dac.decoder.model.1.block.2.block.3.weight_g\", \"dac.decoder.model.1.block.2.block.3.weight_v\", \"dac.decoder.model.1.block.3.block.1.bias\", \"dac.decoder.model.1.block.3.block.1.weight_g\", \"dac.decoder.model.1.block.3.block.1.weight_v\", \"dac.decoder.model.1.block.3.block.3.bias\", \"dac.decoder.model.1.block.3.block.3.weight_g\", \"dac.decoder.model.1.block.3.block.3.weight_v\", \"dac.decoder.model.1.block.4.block.1.bias\", \"dac.decoder.model.1.block.4.block.1.weight_g\", \"dac.decoder.model.1.block.4.block.1.weight_v\", \"dac.decoder.model.1.block.4.block.3.bias\", \"dac.decoder.model.1.block.4.block.3.weight_g\", \"dac.decoder.model.1.block.4.block.3.weight_v\", \"dac.decoder.model.2.block.1.bias\", \"dac.decoder.model.2.block.1.weight_g\", \"dac.decoder.model.2.block.1.weight_v\", \"dac.decoder.model.2.block.2.block.1.bias\", \"dac.decoder.model.2.block.2.block.1.weight_g\", \"dac.decoder.model.2.block.2.block.1.weight_v\", \"dac.decoder.model.2.block.2.block.3.bias\", \"dac.decoder.model.2.block.2.block.3.weight_g\", \"dac.decoder.model.2.block.2.block.3.weight_v\", \"dac.decoder.model.2.block.3.block.1.bias\", \"dac.decoder.model.2.block.3.block.1.weight_g\", \"dac.decoder.model.2.block.3.block.1.weight_v\", \"dac.decoder.model.2.block.3.block.3.bias\", \"dac.decoder.model.2.block.3.block.3.weight_g\", \"dac.decoder.model.2.block.3.block.3.weight_v\", \"dac.decoder.model.2.block.4.block.1.bias\", \"dac.decoder.model.2.block.4.block.1.weight_g\", \"dac.decoder.model.2.block.4.block.1.weight_v\", \"dac.decoder.model.2.block.4.block.3.bias\", \"dac.decoder.model.2.block.4.block.3.weight_g\", \"dac.decoder.model.2.block.4.block.3.weight_v\", \"dac.decoder.model.3.block.1.bias\", \"dac.decoder.model.3.block.1.weight_g\", \"dac.decoder.model.3.block.1.weight_v\", \"dac.decoder.model.3.block.2.block.1.bias\", \"dac.decoder.model.3.block.2.block.1.weight_g\", \"dac.decoder.model.3.block.2.block.1.weight_v\", \"dac.decoder.model.3.block.2.block.3.bias\", \"dac.decoder.model.3.block.2.block.3.weight_g\", \"dac.decoder.model.3.block.2.block.3.weight_v\", \"dac.decoder.model.3.block.3.block.1.bias\", \"dac.decoder.model.3.block.3.block.1.weight_g\", \"dac.decoder.model.3.block.3.block.1.weight_v\", \"dac.decoder.model.3.block.3.block.3.bias\", \"dac.decoder.model.3.block.3.block.3.weight_g\", \"dac.decoder.model.3.block.3.block.3.weight_v\", \"dac.decoder.model.3.block.4.block.1.bias\", \"dac.decoder.model.3.block.4.block.1.weight_g\", \"dac.decoder.model.3.block.4.block.1.weight_v\", \"dac.decoder.model.3.block.4.block.3.bias\", \"dac.decoder.model.3.block.4.block.3.weight_g\", \"dac.decoder.model.3.block.4.block.3.weight_v\", \"dac.decoder.model.4.block.1.bias\", \"dac.decoder.model.4.block.1.weight_g\", \"dac.decoder.model.4.block.1.weight_v\", \"dac.decoder.model.4.block.2.block.1.bias\", \"dac.decoder.model.4.block.2.block.1.weight_g\", \"dac.decoder.model.4.block.2.block.1.weight_v\", \"dac.decoder.model.4.block.2.block.3.bias\", \"dac.decoder.model.4.block.2.block.3.weight_g\", \"dac.decoder.model.4.block.2.block.3.weight_v\", \"dac.decoder.model.4.block.3.block.1.bias\", \"dac.decoder.model.4.block.3.block.1.weight_g\", \"dac.decoder.model.4.block.3.block.1.weight_v\", \"dac.decoder.model.4.block.3.block.3.bias\", \"dac.decoder.model.4.block.3.block.3.weight_g\", \"dac.decoder.model.4.block.3.block.3.weight_v\", \"dac.decoder.model.4.block.4.block.1.bias\", \"dac.decoder.model.4.block.4.block.1.weight_g\", \"dac.decoder.model.4.block.4.block.1.weight_v\", \"dac.decoder.model.4.block.4.block.3.bias\", \"dac.decoder.model.4.block.4.block.3.weight_g\", \"dac.decoder.model.4.block.4.block.3.weight_v\", \"dac.decoder.model.5.block.1.bias\", \"dac.decoder.model.5.block.1.weight_g\", \"dac.decoder.model.5.block.1.weight_v\", \"dac.decoder.model.5.block.2.block.1.bias\", \"dac.decoder.model.5.block.2.block.1.weight_g\", \"dac.decoder.model.5.block.2.block.1.weight_v\", \"dac.decoder.model.5.block.2.block.3.bias\", \"dac.decoder.model.5.block.2.block.3.weight_g\", \"dac.decoder.model.5.block.2.block.3.weight_v\", \"dac.decoder.model.5.block.3.block.1.bias\", \"dac.decoder.model.5.block.3.block.1.weight_g\", \"dac.decoder.model.5.block.3.block.1.weight_v\", \"dac.decoder.model.5.block.3.block.3.bias\", \"dac.decoder.model.5.block.3.block.3.weight_g\", \"dac.decoder.model.5.block.3.block.3.weight_v\", \"dac.decoder.model.5.block.4.block.1.bias\", \"dac.decoder.model.5.block.4.block.1.weight_g\", \"dac.decoder.model.5.block.4.block.1.weight_v\", \"dac.decoder.model.5.block.4.block.3.bias\", \"dac.decoder.model.5.block.4.block.3.weight_g\", \"dac.decoder.model.5.block.4.block.3.weight_v\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m12hz_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m dualcodec_model \u001b[38;5;241m=\u001b[39m \u001b[43mdualcodec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(dualcodec_model)\n",
      "File \u001b[0;32m~/dualcodec-exp/dualcodec/infer/dualcodec/get_model.py:38\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_id, pretrained_model_path, is_checkpoint)\u001b[0m\n\u001b[1;32m     35\u001b[0m         model_fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pretrained_model_path, model_id_to_fname[model_id])\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model from\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_fname)\n\u001b[0;32m---> 38\u001b[0m     \u001b[43msafetensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_fname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/dualcodec-exp/.venv/lib/python3.10/site-packages/safetensors/torch.py:221\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model, filename, strict, device)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unexpected:\n\u001b[1;32m    220\u001b[0m         error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    Unexpected key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munexpected_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m missing, unexpected\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DualCodec:\n    Missing key(s) in state_dict: \"dac.decoder.model.1.block.1.conv.conv.bias\", \"dac.decoder.model.1.block.1.conv.conv.weight_g\", \"dac.decoder.model.1.block.1.conv.conv.weight_v\", \"dac.decoder.model.1.block.2.block.1.conv.bias\", \"dac.decoder.model.1.block.2.block.1.conv.weight_g\", \"dac.decoder.model.1.block.2.block.1.conv.weight_v\", \"dac.decoder.model.1.block.2.block.3.conv.bias\", \"dac.decoder.model.1.block.2.block.3.conv.weight_g\", \"dac.decoder.model.1.block.2.block.3.conv.weight_v\", \"dac.decoder.model.1.block.3.block.1.conv.bias\", \"dac.decoder.model.1.block.3.block.1.conv.weight_g\", \"dac.decoder.model.1.block.3.block.1.conv.weight_v\", \"dac.decoder.model.1.block.3.block.3.conv.bias\", \"dac.decoder.model.1.block.3.block.3.conv.weight_g\", \"dac.decoder.model.1.block.3.block.3.conv.weight_v\", \"dac.decoder.model.1.block.4.block.1.conv.bias\", \"dac.decoder.model.1.block.4.block.1.conv.weight_g\", \"dac.decoder.model.1.block.4.block.1.conv.weight_v\", \"dac.decoder.model.1.block.4.block.3.conv.bias\", \"dac.decoder.model.1.block.4.block.3.conv.weight_g\", \"dac.decoder.model.1.block.4.block.3.conv.weight_v\", \"dac.decoder.model.2.block.1.conv.conv.bias\", \"dac.decoder.model.2.block.1.conv.conv.weight_g\", \"dac.decoder.model.2.block.1.conv.conv.weight_v\", \"dac.decoder.model.2.block.2.block.1.conv.bias\", \"dac.decoder.model.2.block.2.block.1.conv.weight_g\", \"dac.decoder.model.2.block.2.block.1.conv.weight_v\", \"dac.decoder.model.2.block.2.block.3.conv.bias\", \"dac.decoder.model.2.block.2.block.3.conv.weight_g\", \"dac.decoder.model.2.block.2.block.3.conv.weight_v\", \"dac.decoder.model.2.block.3.block.1.conv.bias\", \"dac.decoder.model.2.block.3.block.1.conv.weight_g\", \"dac.decoder.model.2.block.3.block.1.conv.weight_v\", \"dac.decoder.model.2.block.3.block.3.conv.bias\", \"dac.decoder.model.2.block.3.block.3.conv.weight_g\", \"dac.decoder.model.2.block.3.block.3.conv.weight_v\", \"dac.decoder.model.2.block.4.block.1.conv.bias\", \"dac.decoder.model.2.block.4.block.1.conv.weight_g\", \"dac.decoder.model.2.block.4.block.1.conv.weight_v\", \"dac.decoder.model.2.block.4.block.3.conv.bias\", \"dac.decoder.model.2.block.4.block.3.conv.weight_g\", \"dac.decoder.model.2.block.4.block.3.conv.weight_v\", \"dac.decoder.model.3.block.1.conv.conv.bias\", \"dac.decoder.model.3.block.1.conv.conv.weight_g\", \"dac.decoder.model.3.block.1.conv.conv.weight_v\", \"dac.decoder.model.3.block.2.block.1.conv.bias\", \"dac.decoder.model.3.block.2.block.1.conv.weight_g\", \"dac.decoder.model.3.block.2.block.1.conv.weight_v\", \"dac.decoder.model.3.block.2.block.3.conv.bias\", \"dac.decoder.model.3.block.2.block.3.conv.weight_g\", \"dac.decoder.model.3.block.2.block.3.conv.weight_v\", \"dac.decoder.model.3.block.3.block.1.conv.bias\", \"dac.decoder.model.3.block.3.block.1.conv.weight_g\", \"dac.decoder.model.3.block.3.block.1.conv.weight_v\", \"dac.decoder.model.3.block.3.block.3.conv.bias\", \"dac.decoder.model.3.block.3.block.3.conv.weight_g\", \"dac.decoder.model.3.block.3.block.3.conv.weight_v\", \"dac.decoder.model.3.block.4.block.1.conv.bias\", \"dac.decoder.model.3.block.4.block.1.conv.weight_g\", \"dac.decoder.model.3.block.4.block.1.conv.weight_v\", \"dac.decoder.model.3.block.4.block.3.conv.bias\", \"dac.decoder.model.3.block.4.block.3.conv.weight_g\", \"dac.decoder.model.3.block.4.block.3.conv.weight_v\", \"dac.decoder.model.4.block.1.conv.conv.bias\", \"dac.decoder.model.4.block.1.conv.conv.weight_g\", \"dac.decoder.model.4.block.1.conv.conv.weight_v\", \"dac.decoder.model.4.block.2.block.1.conv.bias\", \"dac.decoder.model.4.block.2.block.1.conv.weight_g\", \"dac.decoder.model.4.block.2.block.1.conv.weight_v\", \"dac.decoder.model.4.block.2.block.3.conv.bias\", \"dac.decoder.model.4.block.2.block.3.conv.weight_g\", \"dac.decoder.model.4.block.2.block.3.conv.weight_v\", \"dac.decoder.model.4.block.3.block.1.conv.bias\", \"dac.decoder.model.4.block.3.block.1.conv.weight_g\", \"dac.decoder.model.4.block.3.block.1.conv.weight_v\", \"dac.decoder.model.4.block.3.block.3.conv.bias\", \"dac.decoder.model.4.block.3.block.3.conv.weight_g\", \"dac.decoder.model.4.block.3.block.3.conv.weight_v\", \"dac.decoder.model.4.block.4.block.1.conv.bias\", \"dac.decoder.model.4.block.4.block.1.conv.weight_g\", \"dac.decoder.model.4.block.4.block.1.conv.weight_v\", \"dac.decoder.model.4.block.4.block.3.conv.bias\", \"dac.decoder.model.4.block.4.block.3.conv.weight_g\", \"dac.decoder.model.4.block.4.block.3.conv.weight_v\", \"dac.decoder.model.5.block.1.conv.conv.bias\", \"dac.decoder.model.5.block.1.conv.conv.weight_g\", \"dac.decoder.model.5.block.1.conv.conv.weight_v\", \"dac.decoder.model.5.block.2.block.1.conv.bias\", \"dac.decoder.model.5.block.2.block.1.conv.weight_g\", \"dac.decoder.model.5.block.2.block.1.conv.weight_v\", \"dac.decoder.model.5.block.2.block.3.conv.bias\", \"dac.decoder.model.5.block.2.block.3.conv.weight_g\", \"dac.decoder.model.5.block.2.block.3.conv.weight_v\", \"dac.decoder.model.5.block.3.block.1.conv.bias\", \"dac.decoder.model.5.block.3.block.1.conv.weight_g\", \"dac.decoder.model.5.block.3.block.1.conv.weight_v\", \"dac.decoder.model.5.block.3.block.3.conv.bias\", \"dac.decoder.model.5.block.3.block.3.conv.weight_g\", \"dac.decoder.model.5.block.3.block.3.conv.weight_v\", \"dac.decoder.model.5.block.4.block.1.conv.bias\", \"dac.decoder.model.5.block.4.block.1.conv.weight_g\", \"dac.decoder.model.5.block.4.block.1.conv.weight_v\", \"dac.decoder.model.5.block.4.block.3.conv.bias\", \"dac.decoder.model.5.block.4.block.3.conv.weight_g\", \"dac.decoder.model.5.block.4.block.3.conv.weight_v\"\n    Unexpected key(s) in state_dict: \"dac.decoder.model.1.block.1.bias\", \"dac.decoder.model.1.block.1.weight_g\", \"dac.decoder.model.1.block.1.weight_v\", \"dac.decoder.model.1.block.2.block.1.bias\", \"dac.decoder.model.1.block.2.block.1.weight_g\", \"dac.decoder.model.1.block.2.block.1.weight_v\", \"dac.decoder.model.1.block.2.block.3.bias\", \"dac.decoder.model.1.block.2.block.3.weight_g\", \"dac.decoder.model.1.block.2.block.3.weight_v\", \"dac.decoder.model.1.block.3.block.1.bias\", \"dac.decoder.model.1.block.3.block.1.weight_g\", \"dac.decoder.model.1.block.3.block.1.weight_v\", \"dac.decoder.model.1.block.3.block.3.bias\", \"dac.decoder.model.1.block.3.block.3.weight_g\", \"dac.decoder.model.1.block.3.block.3.weight_v\", \"dac.decoder.model.1.block.4.block.1.bias\", \"dac.decoder.model.1.block.4.block.1.weight_g\", \"dac.decoder.model.1.block.4.block.1.weight_v\", \"dac.decoder.model.1.block.4.block.3.bias\", \"dac.decoder.model.1.block.4.block.3.weight_g\", \"dac.decoder.model.1.block.4.block.3.weight_v\", \"dac.decoder.model.2.block.1.bias\", \"dac.decoder.model.2.block.1.weight_g\", \"dac.decoder.model.2.block.1.weight_v\", \"dac.decoder.model.2.block.2.block.1.bias\", \"dac.decoder.model.2.block.2.block.1.weight_g\", \"dac.decoder.model.2.block.2.block.1.weight_v\", \"dac.decoder.model.2.block.2.block.3.bias\", \"dac.decoder.model.2.block.2.block.3.weight_g\", \"dac.decoder.model.2.block.2.block.3.weight_v\", \"dac.decoder.model.2.block.3.block.1.bias\", \"dac.decoder.model.2.block.3.block.1.weight_g\", \"dac.decoder.model.2.block.3.block.1.weight_v\", \"dac.decoder.model.2.block.3.block.3.bias\", \"dac.decoder.model.2.block.3.block.3.weight_g\", \"dac.decoder.model.2.block.3.block.3.weight_v\", \"dac.decoder.model.2.block.4.block.1.bias\", \"dac.decoder.model.2.block.4.block.1.weight_g\", \"dac.decoder.model.2.block.4.block.1.weight_v\", \"dac.decoder.model.2.block.4.block.3.bias\", \"dac.decoder.model.2.block.4.block.3.weight_g\", \"dac.decoder.model.2.block.4.block.3.weight_v\", \"dac.decoder.model.3.block.1.bias\", \"dac.decoder.model.3.block.1.weight_g\", \"dac.decoder.model.3.block.1.weight_v\", \"dac.decoder.model.3.block.2.block.1.bias\", \"dac.decoder.model.3.block.2.block.1.weight_g\", \"dac.decoder.model.3.block.2.block.1.weight_v\", \"dac.decoder.model.3.block.2.block.3.bias\", \"dac.decoder.model.3.block.2.block.3.weight_g\", \"dac.decoder.model.3.block.2.block.3.weight_v\", \"dac.decoder.model.3.block.3.block.1.bias\", \"dac.decoder.model.3.block.3.block.1.weight_g\", \"dac.decoder.model.3.block.3.block.1.weight_v\", \"dac.decoder.model.3.block.3.block.3.bias\", \"dac.decoder.model.3.block.3.block.3.weight_g\", \"dac.decoder.model.3.block.3.block.3.weight_v\", \"dac.decoder.model.3.block.4.block.1.bias\", \"dac.decoder.model.3.block.4.block.1.weight_g\", \"dac.decoder.model.3.block.4.block.1.weight_v\", \"dac.decoder.model.3.block.4.block.3.bias\", \"dac.decoder.model.3.block.4.block.3.weight_g\", \"dac.decoder.model.3.block.4.block.3.weight_v\", \"dac.decoder.model.4.block.1.bias\", \"dac.decoder.model.4.block.1.weight_g\", \"dac.decoder.model.4.block.1.weight_v\", \"dac.decoder.model.4.block.2.block.1.bias\", \"dac.decoder.model.4.block.2.block.1.weight_g\", \"dac.decoder.model.4.block.2.block.1.weight_v\", \"dac.decoder.model.4.block.2.block.3.bias\", \"dac.decoder.model.4.block.2.block.3.weight_g\", \"dac.decoder.model.4.block.2.block.3.weight_v\", \"dac.decoder.model.4.block.3.block.1.bias\", \"dac.decoder.model.4.block.3.block.1.weight_g\", \"dac.decoder.model.4.block.3.block.1.weight_v\", \"dac.decoder.model.4.block.3.block.3.bias\", \"dac.decoder.model.4.block.3.block.3.weight_g\", \"dac.decoder.model.4.block.3.block.3.weight_v\", \"dac.decoder.model.4.block.4.block.1.bias\", \"dac.decoder.model.4.block.4.block.1.weight_g\", \"dac.decoder.model.4.block.4.block.1.weight_v\", \"dac.decoder.model.4.block.4.block.3.bias\", \"dac.decoder.model.4.block.4.block.3.weight_g\", \"dac.decoder.model.4.block.4.block.3.weight_v\", \"dac.decoder.model.5.block.1.bias\", \"dac.decoder.model.5.block.1.weight_g\", \"dac.decoder.model.5.block.1.weight_v\", \"dac.decoder.model.5.block.2.block.1.bias\", \"dac.decoder.model.5.block.2.block.1.weight_g\", \"dac.decoder.model.5.block.2.block.1.weight_v\", \"dac.decoder.model.5.block.2.block.3.bias\", \"dac.decoder.model.5.block.2.block.3.weight_g\", \"dac.decoder.model.5.block.2.block.3.weight_v\", \"dac.decoder.model.5.block.3.block.1.bias\", \"dac.decoder.model.5.block.3.block.1.weight_g\", \"dac.decoder.model.5.block.3.block.1.weight_v\", \"dac.decoder.model.5.block.3.block.3.bias\", \"dac.decoder.model.5.block.3.block.3.weight_g\", \"dac.decoder.model.5.block.3.block.3.weight_v\", \"dac.decoder.model.5.block.4.block.1.bias\", \"dac.decoder.model.5.block.4.block.1.weight_g\", \"dac.decoder.model.5.block.4.block.1.weight_v\", \"dac.decoder.model.5.block.4.block.3.bias\", \"dac.decoder.model.5.block.4.block.3.weight_g\", \"dac.decoder.model.5.block.4.block.3.weight_v\""
     ]
    }
   ],
   "source": [
    "model_id = \"12hz_v1\"\n",
    "dualcodec_model = dualcodec.get_model(model_id)\n",
    "print(dualcodec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9425097",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "dualcodec_inference = dualcodec.Inference(dualcodec_model=dualcodec_model, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5419f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dac_model = dualcodec_inference.model.dac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inspect.getsource(dualcodec_inference.decode))\n",
    "print(inspect.getsource(dualcodec_inference.model.decode_from_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = prepare_data(max_shards=1)\n",
    "audios = []\n",
    "sample_rates = []\n",
    "for i in range(10):                                          \n",
    "    audio = torch.from_numpy(ds[i][\"mp3\"][\"array\"]).float()  # type: ignore[attr-defined] \n",
    "    sample_rates.append(ds[i][\"mp3\"][\"sampling_rate\"])       # type: ignore[attr-defined]\n",
    "    audios.append(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, sr = torchaudio.load(\"tara.wav\")\n",
    "sample = torchaudio.transforms.Resample(orig_freq=sr, new_freq=24000)(sample)\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c46a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_code, acoustic_code = dualcodec_inference.encode(sample.reshape(1,1,-1), n_quantizers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88889721",
   "metadata": {},
   "outputs": [],
   "source": [
    "dualcodec_inference.model.convnext_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c898cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(semantic_code[0][0]))\n",
    "print(max(acoustic_code[0][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(semantic_code.shape)\n",
    "print(acoustic_code.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eff0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the decoder codes 1-by-1 instead and collect the output samples to see if the decoder works\n",
    "def calculate_audio_with_receptive(semantic_code, acoustic_code, look_ahead, look_back):\n",
    "    my_audio = np.array([])\n",
    "    num_codes = len(semantic_code[0][0])\n",
    "    assert num_codes == len(acoustic_code[0][0])\n",
    "\n",
    "    for i in range(0, num_codes):\n",
    "        l = max(i - look_back, 0)\n",
    "        r = min(i + look_ahead, num_codes)\n",
    "\n",
    "        # print(f\"looking back {l} tokens, and looking ahead {r} tokens\")\n",
    "\n",
    "        sm = semantic_code[:, :, l:r]\n",
    "        ac = acoustic_code[:, :, l:r]\n",
    "\n",
    "        # print(f\"num codes given: {r-l}\")\n",
    "        # print(f\"num samples generated: {out_audio.shape[0]}\")\n",
    "\n",
    "        out_audio = dualcodec_inference.decode(sm, ac)\n",
    "        out_audio = out_audio.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "        space = l * 1920\n",
    "\n",
    "        # print(l, r, len(out_audio), i * 1920 - space)\n",
    "\n",
    "        my_audio = np.concatenate([my_audio, out_audio[1920 * i - space : 1920 * (i+1) - space]])\n",
    "        \n",
    "    return my_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a5f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original audio\n",
    "Audio(sample.squeeze(0).squeeze(0).cpu().numpy(), rate=24000)\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b5ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_streamed_audio = dualcodec_inference.decode(semantic_code, acoustic_code).squeeze(0).cpu().numpy()\n",
    "print(non_streamed_audio.shape)\n",
    "Audio(non_streamed_audio, rate=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f8016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_1 = calculate_audio_with_receptive(semantic_code, acoustic_code, 10, 10)\n",
    "# stream_2 = calculate_audio_with_receptive(semantic_code, acoustic_code, 20, 20)\n",
    "# stream_3 = calculate_audio_with_receptive(semantic_code, acoustic_code, 30, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23981240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_4 = calculate_audio_with_receptive(semantic_code, acoustic_code, 40, 40)\n",
    "# stream_5 = calculate_audio_with_receptive(semantic_code, acoustic_code, 50, 50)\n",
    "# stream_6 = calculate_audio_with_receptive(semantic_code, acoustic_code, 60, 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_7 = calculate_audio_with_receptive(semantic_code, acoustic_code, 70, 70)\n",
    "# stream_8 = calculate_audio_with_receptive(semantic_code, acoustic_code, 80, 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae5c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_9 = calculate_audio_with_receptive(semantic_code, acoustic_code, 5, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_8 = calculate_audio_with_receptive(semantic_code, acoustic_code, 5, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3854cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot(stream_8, label=\"stream_8\")\n",
    "plotter.plot(stream_9, label=\"stream_9\")\n",
    "plotter.legend()\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2789ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(stream_9, rate=24000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(stream_8, rate=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6450426",
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = [\n",
    "    stream_8,\n",
    "    stream_9,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, stream in enumerate(streams):\n",
    "    differences = stream - non_streamed_audio[0]\n",
    "    plotter.plot(differences, label=f\"stream_{j+1}\")\n",
    "    plotter.legend()\n",
    "    plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f60a88",
   "metadata": {},
   "source": [
    "### DAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437cbc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dac_inputs = torch.randn(1, 1024, 252, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ae18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_stream_dac = dac_model.decoder(dac_inputs).squeeze(0).squeeze(0).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f10692",
   "metadata": {},
   "outputs": [],
   "source": [
    "dac_model.decoder_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lol(inputs, model_now, look_ahead, look_back, space_cons):\n",
    "    my_audio = np.array([])\n",
    "    num_codes = len(inputs[0][0])\n",
    "    \n",
    "\n",
    "    for i in range(0, num_codes):\n",
    "        l = max(i - look_back, 0)\n",
    "        r = min(i + look_ahead, num_codes)\n",
    "        \n",
    "        dac_inputs = inputs[:, :, l:r]\n",
    "\n",
    "        out_audio = model_now(dac_inputs)\n",
    "        out_audio = out_audio.squeeze(0).squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "        space = l * space_cons\n",
    "        \n",
    "        # print(f\"num samples generated: {out_audio.shape}\")\n",
    "        # print(f\"my audio shape: {my_audio.shape}\")\n",
    "        \n",
    "        my_audio = np.concatenate([\n",
    "            my_audio, \n",
    "            out_audio[space_cons * i - space : space_cons * (i+1) - space]\n",
    "            ])\n",
    "        \n",
    "        # print(f\"num samples added so far: {my_audio.shape[0]} at loop {i}\")\n",
    "        \n",
    "    return my_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ff335",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamed_dac_audio = lol(dac_inputs, dac_model.decoder, 65, 55, 1920);\n",
    "streamed_dac_audio_2 = lol(dac_inputs, dac_model.decoder, 70, 55, 1920);\n",
    "streamed_dac_audio_3 = lol(dac_inputs, dac_model.decoder, 75, 55, 1920);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73699fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamed_dac_audio_4 = lol(dac_inputs, dac_model.decoder, 55, 75, 1920);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(non_stream_dac.shape)\n",
    "print(streamed_dac_audio.shape)\n",
    "print(streamed_dac_audio_2.shape)\n",
    "print(streamed_dac_audio_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot(streamed_dac_audio_2, label=\"streamed\")\n",
    "plotter.show()\n",
    "plotter.plot(non_stream_dac, label=\"non-streamed\")\n",
    "plotter.legend()\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot(streamed_dac_audio_4 - non_stream_dac, label=\"diff\")\n",
    "plotter.vlines(x=1920 * 75 - 4, ymin=-0.001, ymax=0.001, color=\"red\")\n",
    "plotter.vlines(x=1920 * (252 - 55) - 4, ymin=-0.001, ymax=0.001, color=\"red\")\n",
    "plotter.legend()\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c52bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = dac_model.decoder.model\n",
    "print(layers[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9849bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lol(inputs, model_now, look_ahead, look_back, space_cons, num_frames):\n",
    "    my_audio = np.zeros((inputs.shape[0], inputs.shape[1], num_frames))\n",
    "    \n",
    "    for i in range(0, num_frames):\n",
    "        l = max(i - look_back, 0)\n",
    "        r = min(i + look_ahead, num_frames)\n",
    "        \n",
    "        dac_inputs = inputs[:, :, l:r]\n",
    "\n",
    "        out_audio = model_now(dac_inputs)\n",
    "        out_audio = out_audio.cpu().detach().numpy()\n",
    "        \n",
    "        print(out_audio.shape)\n",
    "\n",
    "        space = l * space_cons\n",
    "        \n",
    "        my_audio[:, :, i] = out_audio[:, :, space_cons * i - space : space_cons * (i+1) - space]\n",
    "        \n",
    "    return my_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "dac_outputs_non_stream_1 = layers[0](dac_inputs)\n",
    "dac_outputs_stream_1 = lol(dac_inputs, layers[0], 1, 1, 1920, dac_inputs.shape[2])\n",
    "\n",
    "\n",
    "\n",
    "print(dac_outputs_non_stream_1.shape)\n",
    "print(dac_outputs_stream_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a64e9d",
   "metadata": {},
   "source": [
    "### Wav2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = dualcodec_inference.semantic_cfg.feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_5s = torch.randn(16000 * 5).numpy()\n",
    "audio_7s = np.concatenate((audio_5s, torch.randn(16000 * 5).numpy()))\n",
    "\n",
    "print(audio_5s.shape)\n",
    "print(audio_7s.shape)\n",
    "\n",
    "o1 = new_model(audio_5s, sampling_rate=16000)[\"input_features\"][0]\n",
    "o2 = new_model(audio_7s, sampling_rate=16000)[\"input_features\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c59fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(o1.shape)\n",
    "print(o2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5afd88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(o1[0][:10])\n",
    "print(\"--------------------------------\")\n",
    "print(o2[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d9919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
