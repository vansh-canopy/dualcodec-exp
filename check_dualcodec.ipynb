{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e1e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dualcodec\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plotter\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460f120a",
   "metadata": {},
   "source": [
    "#### Emilia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b217289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def prepare_data(load_from=\"/mnt/disks/emilia/emilia_dataset/Emilia/EN\", max_shards=1000, num_proc=200):\n",
    "    tar_paths = sorted([filename for filename in os.listdir(load_from) if filename.endswith(\".tar\")])\n",
    "    language = \"en\"\n",
    "\n",
    "    selected_tar_paths = tar_paths[:max_shards]\n",
    "    data_files = {language: selected_tar_paths}\n",
    "\n",
    "    ds = load_dataset(  \n",
    "        load_from,\n",
    "        data_files=data_files,\n",
    "        split=language,\n",
    "        num_proc=num_proc,\n",
    "        cache_dir=\"/mnt/disks/emilia/emilia_cache/\"\n",
    "    )\n",
    "    \n",
    "    return ds.remove_columns([c for c in ds.column_names if c not in [\"mp3\", \"json\"]])  # type: ignore[attr-defined]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c1d3ca",
   "metadata": {},
   "source": [
    "#### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59daa7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"12hz_v1\"\n",
    "\n",
    "path_causal = \"output_checkpoints_2/dualcodec_experiments_fully_causal/checkpoint/epoch-0000_step-0118000_loss-118.932007-dualcodec_experiments_fully_causal\"\n",
    "\n",
    "path_look_ahead = \"output_checkpoints_3/dualcodec_experiments_look_ahead/checkpoint/epoch-0000_step-0097000_loss-109.211716-dualcodec_experiments_look_ahead\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67031f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dualcodec_model = dualcodec.get_model(model_id, path_causal, is_checkpoint=True)\n",
    "dualcodec_inference = dualcodec.Inference(dualcodec_model=dualcodec_model, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9425097",
   "metadata": {},
   "outputs": [],
   "source": [
    "dac_model = dualcodec_inference.model.dac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad4a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dualcodec.model_codec.dac_model import DAC\n",
    "\n",
    "dac_model = DAC(\n",
    "    encoder_rates=[2, 4, 8, 8],\n",
    "    latent_dim=1024,\n",
    "    decoder_dim=1536,\n",
    "    decoder_rates=[8, 8, 4, 2],\n",
    "    n_codebooks=9,\n",
    "    make_dac_causal=True,\n",
    "    add_dac_look_ahead=False,\n",
    ").to(\"cuda\").to(torch.float64)\n",
    "\n",
    "dac_model.eval()\n",
    "print(dac_model.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = prepare_data(max_shards=1)\n",
    "# audios = []\n",
    "# sample_rates = []\n",
    "# for i in range(10):                                          \n",
    "#     audio = torch.from_numpy(ds[i][\"mp3\"][\"array\"]).float()  # type: ignore[attr-defined] \n",
    "#     sample_rates.append(ds[i][\"mp3\"][\"sampling_rate\"])       # type: ignore[attr-defined]\n",
    "#     audios.append(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, sr = torchaudio.load(\"audio_samples/tara.wav\")\n",
    "sample = torchaudio.transforms.Resample(orig_freq=sr, new_freq=24000)(sample)\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c46a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_code, acoustic_code = dualcodec_inference.encode(sample.reshape(1,1,-1), n_quantizers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c898cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(semantic_code[0][0]))\n",
    "print(max(acoustic_code[0][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(semantic_code.shape)\n",
    "print(acoustic_code.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eff0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the decoder codes 1-by-1 instead and collect the output samples to see if the decoder works\n",
    "def calculate_audio_with_receptive(semantic_code, acoustic_code, look_ahead, look_back):\n",
    "    my_audio = np.array([])\n",
    "    num_codes = len(semantic_code[0][0])\n",
    "    assert num_codes == len(acoustic_code[0][0])\n",
    "\n",
    "    for i in range(0, num_codes):\n",
    "        l = max(i - look_back, 0)\n",
    "        r = min(i + look_ahead, num_codes)\n",
    "\n",
    "        # print(f\"looking back {l} tokens, and looking ahead {r} tokens\")\n",
    "\n",
    "        sm = semantic_code[:, :, l:r]\n",
    "        ac = acoustic_code[:, :, l:r]\n",
    "\n",
    "        # print(f\"num codes given: {r-l}\")\n",
    "        # print(f\"num samples generated: {out_audio.shape[0]}\")\n",
    "\n",
    "        out_audio = dualcodec_inference.decode(sm, ac)\n",
    "        out_audio = out_audio.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "        space = l * 1920\n",
    "\n",
    "        # print(l, r, len(out_audio), i * 1920 - space)\n",
    "\n",
    "        my_audio = np.concatenate([my_audio, out_audio[1920 * i - space : 1920 * (i+1) - space]])\n",
    "        \n",
    "    return my_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a5f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original audio\n",
    "Audio(sample.squeeze(0).squeeze(0).cpu().numpy(), rate=24000)\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b5ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_streamed_audio = dualcodec_inference.decode(semantic_code, acoustic_code).squeeze(0).cpu().numpy()\n",
    "print(non_streamed_audio.shape)\n",
    "Audio(non_streamed_audio, rate=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f8016",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_1 = calculate_audio_with_receptive(semantic_code, acoustic_code, 10, 10)\n",
    "stream_2 = calculate_audio_with_receptive(semantic_code, acoustic_code, 20, 20)\n",
    "stream_3 = calculate_audio_with_receptive(semantic_code, acoustic_code, 30, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3854cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot(stream_1, label=\"stream_8\")\n",
    "plotter.plot(stream_2, label=\"stream_9\")\n",
    "plotter.plot(non_streamed_audio[0], label=\"non-stream\")\n",
    "plotter.legend()\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6450426",
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = [\n",
    "    stream_1,\n",
    "    stream_2,\n",
    "    stream_3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, stream in enumerate(streams):\n",
    "    differences = stream - non_streamed_audio[0]\n",
    "    plotter.plot(differences, label=f\"stream_{j+1}\")\n",
    "    plotter.legend()\n",
    "    plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f60a88",
   "metadata": {},
   "source": [
    "#### DAC Causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437cbc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "k_prime = 1\n",
    "inputs_dac = torch.randn(1, 1024, k, device=\"cuda\", dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_dac_longer = torch.concatenate([\n",
    "    inputs_dac,\n",
    "    torch.randn(1, 1024, 1, device=\"cuda\", dtype=torch.float64)\n",
    "], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77cab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs_dac_longer.shape)\n",
    "print(inputs_dac.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = dac_model.decoder(inputs_dac)\n",
    "    outputs_longer = dac_model.decoder(inputs_dac_longer)\n",
    "\n",
    "print(outputs[0][0][:k])\n",
    "print(outputs_longer[0][0][:k])\n",
    "\n",
    "print(\"-----------\")\n",
    "\n",
    "print(outputs[0][0][:k] - outputs_longer[0][0][:k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1bc79",
   "metadata": {},
   "source": [
    "### DAC Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ae18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_streamed_dac_outputs = dac_model.decoder(inputs_dac).squeeze(0).squeeze(0).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dac_streamer(inputs, model, look_ahead, look_back, space_cons=1920):\n",
    "    my_audio = np.array([])\n",
    "    num_codes = len(inputs[0][0])\n",
    "    \n",
    "\n",
    "    for i in range(0, num_codes):\n",
    "        l = max(i - look_back, 0)\n",
    "        r = min(i + look_ahead, num_codes)\n",
    "        \n",
    "        dac_inputs = inputs[:, :, l:r]\n",
    "\n",
    "        out_audio = model(dac_inputs)\n",
    "        out_audio = out_audio.squeeze(0).squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "        space = l * space_cons\n",
    "        \n",
    "        print(f\"num samples generated: {out_audio.shape}\")\n",
    "        print(f\"my audio shape: {my_audio.shape}\")\n",
    "        \n",
    "        my_audio = np.concatenate([\n",
    "            my_audio, \n",
    "            out_audio[space_cons * i - space : space_cons * (i+1) - space]\n",
    "            ])\n",
    "        \n",
    "        print(f\"num samples added so far: {my_audio.shape[0]} at loop {i}\")\n",
    "        \n",
    "    return my_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ff335",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamed_dac_outputs = dac_streamer(inputs_dac, dac_model.decoder, 65, 0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(non_streamed_dac_outputs.shape)\n",
    "print(streamed_dac_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot(streamed_dac_outputs, label=\"streamed\")\n",
    "plotter.show()\n",
    "plotter.plot(non_streamed_dac_outputs, label=\"non-streamed\")\n",
    "plotter.legend()\n",
    "plotter.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
